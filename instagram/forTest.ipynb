{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# class JsonWriterPipeline(object):\n",
    "\n",
    "#     def open_spider(self, spider):\n",
    "#         self.file = open('test.json', 'w')\n",
    "\n",
    "#     def close_spider(self, spider):\n",
    "#         self.file.close()\n",
    "\n",
    "#     def process_item(self, item, spider):\n",
    "#         line = json.dumps(dict(item)) + \"\\n\"\n",
    "#         self.file.write(line)\n",
    "#         return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-31 00:01:11 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: instagram)\n",
      "2019-08-31 00:01:11 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 26 2018, 23:26:24) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Darwin-18.7.0-x86_64-i386-64bit\n",
      "2019-08-31 00:01:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'instagram', 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'tag_힐링.json', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'instagram.spiders', 'RETRY_HTTP_CODES': [429], 'SPIDER_MODULES': ['instagram.spiders']}\n",
      "2019-08-31 00:01:11 [scrapy.extensions.telnet] INFO: Telnet Password: e7def489cbc50de6\n",
      "2019-08-31 00:01:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-08-31 00:01:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'instagram.middlewares.TooManyRequestsRetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy_splash.SplashCookiesMiddleware',\n",
      " 'scrapy_splash.SplashMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-08-31 00:01:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-08-31 00:01:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-08-31 00:01:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-08-31 00:01:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-08-31 00:01:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.instagram.com/ajax/bz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-31 00:02:11 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 17 pages/min), scraped 1799 items (at 1799 items/min)\n",
      "2019-08-31 00:03:11 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 17 pages/min), scraped 4002 items (at 2203 items/min)\n",
      "2019-08-31 00:04:06 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.instagram.com/graphql/query/?query_hash=174a5243287c5f3a7de741089750ab3b&variables=%7B%22tag_name%22%3A%22%EC%9E%85%ED%95%99%22%2C%22first%22%3A100%2C%22after%22%3A%22QVFDMk8xOUpibm04X1N0R19Ob08xdHp6WlVwOUVaS21oVC1DRFI3M1Q4M05UN1R4bjcyTGNpbmlDYTNvN2EtTVhIZE05YnZJSUhqZ2oxMjBNMngtSnEtNw%3D%3D%22%7D. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests\n",
      "2019-08-31 00:04:11 [scrapy.extensions.logstats] INFO: Crawled 50 pages (at 16 pages/min), scraped 6148 items (at 2146 items/min)\n",
      "2019-08-31 00:05:11 [scrapy.extensions.logstats] INFO: Crawled 67 pages (at 17 pages/min), scraped 8630 items (at 2482 items/min)\n",
      "2019-08-31 00:06:11 [scrapy.extensions.logstats] INFO: Crawled 78 pages (at 11 pages/min), scraped 10235 items (at 1605 items/min)\n",
      "2019-08-31 00:07:11 [scrapy.extensions.logstats] INFO: Crawled 93 pages (at 15 pages/min), scraped 12455 items (at 2220 items/min)\n",
      "2019-08-31 00:08:11 [scrapy.extensions.logstats] INFO: Crawled 110 pages (at 17 pages/min), scraped 14974 items (at 2519 items/min)\n",
      "2019-08-31 00:09:11 [scrapy.extensions.logstats] INFO: Crawled 126 pages (at 16 pages/min), scraped 17360 items (at 2386 items/min)\n",
      "2019-08-31 00:10:11 [scrapy.extensions.logstats] INFO: Crawled 142 pages (at 16 pages/min), scraped 19648 items (at 2288 items/min)\n",
      "2019-08-31 00:11:11 [scrapy.extensions.logstats] INFO: Crawled 157 pages (at 15 pages/min), scraped 21753 items (at 2105 items/min)\n",
      "2019-08-31 00:12:11 [scrapy.extensions.logstats] INFO: Crawled 174 pages (at 17 pages/min), scraped 24107 items (at 2354 items/min)\n",
      "2019-08-31 00:13:11 [scrapy.extensions.logstats] INFO: Crawled 189 pages (at 15 pages/min), scraped 26159 items (at 2052 items/min)\n",
      "2019-08-31 00:14:11 [scrapy.extensions.logstats] INFO: Crawled 205 pages (at 16 pages/min), scraped 28397 items (at 2238 items/min)\n",
      "2019-08-31 00:15:11 [scrapy.extensions.logstats] INFO: Crawled 221 pages (at 16 pages/min), scraped 30519 items (at 2122 items/min)\n",
      "2019-08-31 00:16:11 [scrapy.extensions.logstats] INFO: Crawled 234 pages (at 13 pages/min), scraped 32319 items (at 1800 items/min)\n",
      "2019-08-31 00:17:11 [scrapy.extensions.logstats] INFO: Crawled 251 pages (at 17 pages/min), scraped 34745 items (at 2426 items/min)\n",
      "2019-08-31 00:18:11 [scrapy.extensions.logstats] INFO: Crawled 268 pages (at 17 pages/min), scraped 37259 items (at 2514 items/min)\n",
      "2019-08-31 00:19:11 [scrapy.extensions.logstats] INFO: Crawled 281 pages (at 13 pages/min), scraped 39194 items (at 1935 items/min)\n",
      "2019-08-31 00:20:11 [scrapy.extensions.logstats] INFO: Crawled 296 pages (at 15 pages/min), scraped 41430 items (at 2236 items/min)\n",
      "2019-08-31 00:21:11 [scrapy.extensions.logstats] INFO: Crawled 312 pages (at 16 pages/min), scraped 43738 items (at 2308 items/min)\n",
      "2019-08-31 00:22:11 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 14 pages/min), scraped 45783 items (at 2045 items/min)\n",
      "2019-08-31 00:23:11 [scrapy.extensions.logstats] INFO: Crawled 340 pages (at 14 pages/min), scraped 47882 items (at 2099 items/min)\n",
      "2019-08-31 00:24:11 [scrapy.extensions.logstats] INFO: Crawled 355 pages (at 15 pages/min), scraped 50130 items (at 2248 items/min)\n",
      "2019-08-31 00:25:11 [scrapy.extensions.logstats] INFO: Crawled 370 pages (at 15 pages/min), scraped 52380 items (at 2250 items/min)\n",
      "2019-08-31 00:26:11 [scrapy.extensions.logstats] INFO: Crawled 386 pages (at 16 pages/min), scraped 54777 items (at 2397 items/min)\n",
      "2019-08-31 00:27:11 [scrapy.extensions.logstats] INFO: Crawled 402 pages (at 16 pages/min), scraped 57177 items (at 2400 items/min)\n",
      "2019-08-31 00:28:11 [scrapy.extensions.logstats] INFO: Crawled 416 pages (at 14 pages/min), scraped 59277 items (at 2100 items/min)\n",
      "2019-08-31 00:29:11 [scrapy.extensions.logstats] INFO: Crawled 433 pages (at 17 pages/min), scraped 61826 items (at 2549 items/min)\n",
      "2019-08-31 00:30:11 [scrapy.extensions.logstats] INFO: Crawled 448 pages (at 15 pages/min), scraped 63996 items (at 2170 items/min)\n",
      "2019-08-31 00:31:11 [scrapy.extensions.logstats] INFO: Crawled 461 pages (at 13 pages/min), scraped 65915 items (at 1919 items/min)\n",
      "2019-08-31 00:32:11 [scrapy.extensions.logstats] INFO: Crawled 469 pages (at 8 pages/min), scraped 67114 items (at 1199 items/min)\n",
      "2019-08-31 00:32:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 https://www.instagram.com/graphql/query/?query_hash=174a5243287c5f3a7de741089750ab3b&variables=%7B%22tag_name%22%3A%22%EC%9E%85%ED%95%99%22%2C%22first%22%3A100%2C%22after%22%3ANone%7D>: HTTP status code is not handled or not allowed\n",
      "2019-08-31 00:32:29 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-08-31 00:32:29 [scrapy.extensions.feedexport] INFO: Stored json feed (67849 items) in: tag_힐링.json\n",
      "2019-08-31 00:32:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 1,\n",
      " 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,\n",
      " 'downloader/request_bytes': 467674,\n",
      " 'downloader/request_count': 476,\n",
      " 'downloader/request_method_count/GET': 474,\n",
      " 'downloader/request_method_count/POST': 2,\n",
      " 'downloader/response_bytes': 37127318,\n",
      " 'downloader/response_count': 475,\n",
      " 'downloader/response_status_count/200': 474,\n",
      " 'downloader/response_status_count/400': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 8, 30, 15, 32, 29, 653255),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/400': 1,\n",
      " 'item_scraped_count': 67849,\n",
      " 'log_count/INFO': 42,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 99164160,\n",
      " 'memusage/startup': 69632000,\n",
      " 'request_depth_max': 474,\n",
      " 'response_received_count': 475,\n",
      " 'retry/count': 1,\n",
      " 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,\n",
      " 'scheduler/dequeued': 478,\n",
      " 'scheduler/dequeued/memory': 478,\n",
      " 'scheduler/enqueued': 478,\n",
      " 'scheduler/enqueued/memory': 478,\n",
      " 'splash/render.html/request_count': 2,\n",
      " 'splash/render.html/response_count/200': 2,\n",
      " 'start_time': datetime.datetime(2019, 8, 30, 15, 1, 11, 875301)}\n",
      "2019-08-31 00:32:29 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "process.crawl('instagramCrawl')\n",
    "process.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
